{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-procesamiento de datos\n",
    "\n",
    "* En este notebook veremos algunas herramientas y líneas de código para el preprocesamiento de datos para su posterior uso en Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar librerías\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar el dataset\n",
    "df = pd.read_csv('00-datasets/Data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se va a trabajar con un dataset de información salarial de personas con diferente edad y nacionalidad y el objetivo de este ejercicio sencillo es predecir si van a compar o no basados en los datos existentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head() # Observamos los primeros 5 registros (exploración de datos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos identificar que tenemos las variables X y Y, donde X son las variables independientes (País, Edad, Salario) y Y la variable dependiente que corresponde al campo Purchased, que será la variable objetivo para identificar si realizará o no una compra el cliente, basado en sus características (variables independientes) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:,:-1].values # Seleccionamos los campos y registros de las variables independientes excepto la columna Purchased (variable dependiente)\n",
    "y = df.iloc[:,3].values # Seleccionamos la columna dependiente (Purchased). Otra opción: y = df.iloc[:,-1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observa que tenemos datos NaN que son datos no conocidos o faltantes, para ellos se pueden realizar varios procedimientos:\n",
    "* Eliminar los registros (filas)\n",
    "* Eliminar los campos (columnas)\n",
    "* Imputación de valores\n",
    "\n",
    "Cada caso depende del contexto de los datos y su naturaleza, para este caso específico, se realizará imputación de datos a través del valor promedio de todos los registros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manejo de datos faltantes\n",
    "from sklearn.impute import SimpleImputer # Importar el módulo necesario para la imputación datos faltantes (En este caso SimpleImputer de Scikit-Learn)\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')  # Crear una instancia de SimpleImputer con una estrategia para reemplazar los valores faltantes con la media\n",
    "imputer.fit(X[:, 1:3]) # Se indica las columnas numéricas a las que le realizará la imputación (Age, Salary)\n",
    "X[:, 1:3] = imputer.transform(X[:, 1:3]) # Transformar y reemplazar los valores faltantes en las columnas especificadas con la media\n",
    "print(X) #Ver el resultado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente paso es analizar los datos categóricos, en este caso la columna Country\n",
    "* El primer paso es codificar los datos categóricos de Country, debido a que para hacer una análisis se necesitan valores numéricos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codificar datos categóricos (Este paso normalemente le llaman creacción de variables Dummy)\n",
    "from sklearn.compose import ColumnTransformer # Importar el módulo necesario para la transormación de columnas (En este caso ColumnTransformer de Scikit-Learn)\n",
    "from sklearn.preprocessing import OneHotEncoder # Importar el módulo necesario para la codificación (En este caso OneHotEncoder de Scikit-Learn)\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0])], remainder='passthrough') # Crear una instancia de ColumnTransformer con la especificación de la codificación One-Hot en la columna 0 y 'remainder' para mantener las demás columnas\n",
    "X = np.array(ct.fit_transform(X)) # Transformar y codificar los datos utilizando la configuración definida en ColumnTransformer\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* El segundo paso es codificar la columna objetivo (Purchased), donde sus valores son Si o No"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codificar la variable Purchased (dependiente)\n",
    "from sklearn.preprocessing import LabelEncoder # Importar el módulo necesario para la codificación (En este caso LabelEncoder de Scikit-Learn), nótese que como son solo dos valores, no es necesario OneHotEncoder\n",
    "le = LabelEncoder() # Se crea una instancia para el codificador\n",
    "y = le.fit_transform(y) # Aplicamos la transformación a la columna y (Purchased)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teniendo hecha la limpieza de los datos y su respectiva codifición, los siguientes pasos corresponden a la etapa de modelado, donde:\n",
    "* Se realiza un split o division de los datos para obtener un porcentaje de datos a entrenar (train) y otro para evaluar (test) (Normalente se usa 80%-20%, 70%-30%, todo depende del contexto,naturaleza de los datos y experticia del profesional)\n",
    "* Escalado de variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Dividir el dataset en un conjuntos de entrenamiento (training) y otro de evaluación (testing)\n",
    "\n",
    "from sklearn.model_selection import train_test_split # Importar el módulo necesario para la división (En este caso model_selection de Scikit-Learn).\n",
    "#Se crean las cuatro variables para el train & test y se le indica la variables independientes (X) y la variable obejtico o dependiente (y).\n",
    "# En test_size: 0.20 se está indicando que el 20% de los datos serán usados para el testing.\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1) # random_state: Se obliga al modelo que use la misma semilla, para evitar diferentes resultados cada vez que se ejecute.\n",
    "\n",
    "# Mostramos las variables de train y test\n",
    "print(X_train)\n",
    "print(X_test)\n",
    "print(y_train)\n",
    "print(y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se mencionó anteriormente, es necesario hacer una escalamiento de las variables a modelar, debido a que hay valores que no se podrían comparar debido a sus dimensiones y esto puede generar sesgos.\n",
    "\n",
    "En este dataset, se observa que la edad maneja una dimension de 0 a 100 apróximadamente, mientras que el salario se encuentra en cientos de miles, por lo tanto cualquier análisis, se vería fuertemente inclinado por el tema salarial, sin importar la edad, es por eso que se suelen usar varias estrategias, dos de ellas son:\n",
    "* Normalización, donde se ajustan los datos en una escala de 0 a 1.\n",
    "* Estandarización, que depende de la media y sus desviación. -1 a 1\n",
    "\n",
    "En este caso, usaremos la Estandarización, para trabajar una misma escala de medida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escalado de variables\n",
    "from sklearn.preprocessing import StandardScaler # # Importar el módulo necesario para la Estandarización (En este caso StandarScaler de Scikit-Learn).\n",
    "sc = StandardScaler()  # Se crea una instancia para la estandarización\n",
    "X_train[:, 3:] = sc.fit_transform(X_train[:, 3:]) # Se aplica la transformación a Age y Salary solamente, debido a que las otras variables están en rangos de 0 y 1\n",
    "X_test[:, 3:] = sc.transform(X_test[:, 3:]) # Se aplica la transformación a X_test, nótese que no se usa fit, esto es porque queremos que aplique la misma transformación hecha a X_train\n",
    "print(X_train)\n",
    "print(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
